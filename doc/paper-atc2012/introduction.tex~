% Introduction

\section{Introduction}
 
This paper describes Syndicate, a wide-area distributed file system
optimized for scalable read performance. Syndicate is unique in that
it purposely decouples file persistence from read performance---it assumes
file persistence is implemented by some other system,
but no matter how poor that system's read performance might be, it
delivers good read performance to a scalable number of clients.
 
For example, data might be stored in a cloud or digital vault that offers
limited and expensive read access (e.g., Amazon S3), and then delivered to
hundreds or thousands of clients through Syndicate. As another
example, data might be stored on a laptop with a 10-Mbps wireless
interface, backed up on an external hard drive, and delivered to
clients via Syndicate.
 
Syndicate is primarily targeted at users that have large data sets that
they want to distribute to a large set of clients through a file
system interface. Some clients read the entire data set, while others
read (and process) only subsets of the data. In some cases a single
writer generates the data, while in others there are multiple writers,
each generating disjoint subsets of the data.
 
Despite being optimized for scalable read performance, Syndicate is a
full-featured distributed file system, albeit one that does not
enhance the persistence of content it distributes. Specifically,
Syndicate is designed to have the following properties:
 
\begin{description}
 
\item[Low Delay:] File operations should incur minimal overhead.
\item[Scalable Throughput:] Aggregate read throughput should scale
with the number of clients.
\item[High Consistency:] A file's data and metadata should remain
consistent despite multiple readers and writers.
\item[High Availability:] A file made accessible through the
filesystem should be able to tolerate multiple network failures.
\item[Scalable Capacity:] There should be no limit to the number
of hosts, files, or bytes of data in the file system.
 
\end{description}
 
Syndicate achieves some of these properties by leverging a content
distribution network (CDN) as its underlying data delivery mechanism.
A CDN implements a set of caches distributed across the network, and
these caches can be used to both (1) move file data closer to clients
(thereby reducing client-perceived latency), and (2) aggregate
delivery capacity across multiple caches (thereby scaling bandwidth
without putting any additional burden on the source).
 
The primary challenge is maintaining consistency in the face of
writes. This is because the underlying CDN generally assumes content
is immutable once available, and typically assumes a single content writer.  
Despite this constraint,
Syndicate provides at worst close-to-open file consistency even
with multiple writers across multiple hosts.
 
This paper describes both how Syndicate leverages a CDN to provide
scalable read performance, yet provides the semantics of a traditional
distributed file system. Sections 2 and 3 described Syndicate's design
and implementation. The paper concludes by evaluating Syndicate's
performance and comparing Syndicate to related work.

\comment{ 
% TAKEAWAY 1: Syndicate is a distributed read/write filesystem that works best for distributing large datasets. 
% 
% TAKEAWAY 2: We want to convince the reader here that the best way to build a distributed filesystem with our given requirements is to use a CDN to help distribute and cache chunks of data. 

% What do we want from a distributed filesystem?  The semantics and behavior of a local filesystem.
Perhaps the most widely used programming interface for indexing, retrieving, and storing information on computer systems is the filesystem interface~\cite{citation needed}.  One prominent~\cite{NFS}~\cite{AFS}~\cite{Ceph}~\cite{Shark}~\cite{Lustre}~\cite{HDFS}~\cite{LFS}~\cite{WheelFS} class of filesystems are distributed filesystems, which present data stored across networked computers as a single locally-mounted filesystem.
\\
Most of these systems attempt to preserve as much of the semantics and behaviors of singular (non-distributed) filesystems as possible.  To do so, a distributed filesystem must mitigate the impact of faults in the network and remote hosts on the availability of data.  As a result, we identify five traits distributed filesystems must strive to maintain to achieve this end:

% What properties do we desire?
\begin{itemize}
\item \textit{Low latency}.  File operations should start as quickly as possible. \\
\item \textit{High bandwidth}.  Read and write operations should occur as quickly as possible. \\
\item \textit{High consistency}.  A file's data and metadata should be as consistent as possible with the operations that have occurred on it. \\
\item \textit{High availability}.  A file exposed in the filesystem namespace should be accessible as often as possible. \\
\item \textit{High scalability}.  The filesystem should allow for as many hosts, files, and bytes of data represented as possible.
\end{itemize}

% Using a CDN can help us achieve some of these traits
One way to achieve some of these traits is to leverage a content distribution network to drive data delivery to readers.  A content distribution network (CDN) is a network of servers (CDN nodes) that cache replicas of content from one or more origin servers in order to increase the access bandwidth and redundancy while reducing the access latency~\cite{citation needed} of the content.  In practice, CDNs are a piece of an Internet content provider's infrastructure that gets used mitigate the effect of flash crowds accessing popular content~\cite{citation needed}.
\\
% What are the consequences of using a CDN in a filesystem?
One immediate consequence of using a CDN is that the read latency for a frequently-read but remotely-hosted file would decrease considerably, since most reads would be directed to a nearby replica.  Read bandwidth on remote files would increase noticeably as well, since multiple replicas may be queried simultaneously and most queries would not be directed to the origin server.  However, the file would be expected to change infrequently on the origin, since a change would need to propagate to the CDN nodes and then to the clients.  Additionally, if the CDN is designed to replicate fragments of content, changing the file on the origin server may introduce file corruption if either the CDN or the client attempts to reassemble the file from both stale and fresh fragments.
\\
% What is Syndicate?  Why is it novel?
To address these problems, we created Syndicate, a scalable, distributed read/write filesystem that explicitly leverages and, if possible, controls a CDN to drive file data delivery between its clients.  Syndicate is designed to allow users to publish and distribute large quantities of data across the Internet using a filesystem interface.  To the extent of our knowledge, it is the first filesystem to leverage a CDN in this manner while addressing the difficulties of using one.
\\
% Paper outline
The remainder of this paper is organized as follows:
}

