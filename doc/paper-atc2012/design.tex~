% Design

\section{Design}
% TAKEAWAY 1: The reader should be able to see the logical divisions in the Syndicate architecture, and how they fit together. 
% \\
% TAKEAWAY 2: The reader should see why we made the architectural decisions we did, given our requirements and constraints.
% \\

\begin{figure}[h!]
\centering
\includegraphics[width=0.5\textwidth]{figures/overview}
\caption{Structural overview of a Syndicate filesystem with two clients.  The ``purge content" operation is an optional CDN feature that Syndicate can use.}
\label{write-operation}
\end{figure}


There are three components in a Syndicate filesystem:  the CDN, the filesystem client, and the metadata server.  The CDN is responsible for caching file data transferred between clients, and must meet a set of requirements in order to be useable to Syndicate.  A client is responsible for presenting the filesystem namespace and metadata to users, hosting locally-created files, retrieving data from remotely-hosted files, and informing the metadata server of local metadata changes.  A metadata server is responsible for maintaining the metadata of a filesystem hierarchy, authorizing metadata operations, resolving write-conflicts, and ensuring only content from valid URLs are cached in the CDN.

Each client by default serves files created by local writers.  The locally-hosted files in a client connected to a metadata server are visible to the metadata server and (depending on access permissions) to each other connected client.  Consequently, the data of the filesystem is logically partitioned across the set of clients and possibly non-client servers across the Internet, and it is assumed that the data for each file is accessible via at most one URL.  Additional file persistence measures such as backups or mirroring are left to other software systems.

It is assumed that the clocks on all hosts in a Syndicate filesystem are loosely synchronized, and have a clock skew of less than a second.  In implementation, we simply run NTPv4 daemons on all hosts to make this assumption true.

\subsection{The CDN}
% NOTE: the use of CoBlitz, in my humble opinion, is an implementation detail that should
% be described in the Implementation section.  There would be a subsection that gives an overview
% of why CoBlitz is the best CDN for the job (since it meets these capabilities)

% Syndicate's use of a CDN is agnostic
Our goal in the design of Syndicate's CDN interface is to make Syndicate as CDN-agnostic as possible.  It should treat a CDN as a piece of programmable network infrastructure invisible to users rather than a first-class filesystem component, and make as few assumptions about its caching behavior as possible.  We describe these assumptions in this section, and derive a driver model for Syndicate to use a CDN that meets them.

% What do we need from our CDN?
First, we assume that all content replicas within the CDN to be addressible by exactly one URL, which we denote $U_{CDN}$.  Syndicate, like a web browser, should not be responsible for tracking individual replicas within the CDN.  We require the CDN to resolve $U_{CDN}$ to an IP address from which to download the bytes of the requested content.

Second, we assume that the CDN expects the content it caches to be immutable.  Put another way, we don't trust the CDN to realize that it may be caching multiple versions of the same piece of content.  Thus, Syndicate must generate a new file URL for the CDN to cache whenever that file changes.  But to do this, Syndicate requires an injective function $T$ from the CDN that maps a given web URL $U$ to $U_{CDN}$ for any valid $U$.

Third, we assume the CDN honors a ``do not cache" hint by not caching files that Syndicate tags with it.  This is because the Syndicate client will know immediately when a file has been written to, and will need to inform the CDN that any data requested from old URLs to the file should not be stored.

% the driver model
To drive a CDN, we require the only the client to know $T$ and to tag responses to stale file URLs with the ``do not cache" hint.

\subsection{The Client}

% tasks: present a metadata server's tree, store local files, serve files
The Syndicate client is a filesystem driver.  It generates the filesystem hierarchy by ``mounting" a metadata server, which is to say it authenticates with the metadata server, downloads its filesystem hierarchy, and periodically refreshes the hierarchy so as to create an eventually-consistent view of all other files across all other clients attached to the same metadata server.  The set of metadata attributes for each file can be found in Table~\ref{metadata-table}.

% authorization
The client has a Syndicate-wide owner ID used to identify file ownership in the filesystem.  It occupies the user ID field in a UNIX system, but refers to the client that last wrote to the file (as opposed to the client that created it).  The owner ID is associated with a user login and password that the client uses to authenticate with a metadata server, in order to gain read and write permission and secure metadata transfer.

The metadata server has a globally unique ID that is analogous to a group ID on a UNIX system.  All files indexed by a metadata server have the same metadata server ID, so clients connected a metadata server are implicitly members of the same user group that owns all the files in a mounted Syndicate filesystem.  In practice, owner IDs and metadata server IDs are selected to be different from all other user and group IDs of all systems participating in the filesystem.

File operations are policed using similar access controls to a conventional UNIX filesystem.  Each file has read, write, and execute bits for the owner, the users connected to the same metadata server, and all users in the world.  Both the client and metadata server will restrict access to file data and metadata accordingly, so rogue user cannot read or write to the filesystem without proper user credentials and permissions.

% metadata
\begin{table*}[ht!]
\label{metadata-table}
\begin{tabular}{ | l | p{14cm} |}
\hline
\textbf{Name} & \textbf{Description} \\
\hline
\texttt{path} & The path to the file relative to the mountpoint \\
\texttt{url}  & The URL to the file on the origin server (may be a local file URL) \\
\texttt{owner} & The ID of the client that hosts this file \\
\texttt{volume} & The ID of the metadata server that indexes this file \\
\texttt{mode} & UNIX permission bits \\
\texttt{size} & The number of bytes in this file \\
\texttt{mtime} & The time of last modification (seconds) \\
\texttt{atime} & (Clients only) the time of last access (seconds) \\
\texttt{ctime} & (Clients only) the time of file creation (seconds) \\
\texttt{checksum} & Cryptographic hash of the file contents. \\
\hline
\end{tabular}
\caption{Summary of metadata fields kept for each file in the Syndicate filesystem.}
\end{table*}

% TODO: do we need a figure of this?  Or is the explanation simple enough to follow?
The client periodically refreshes the metadata by only downloading a directory's metadata from the metadata server if that directory's entries have changed.  We use a Merkel tree~\cite{merkel-trees} built out of the filesystem directories, using the \texttt{checksum} field to quickly identify which directories have changed on the metadata server since the last refresh.  Because the client always has file metadata for each file in the mountpoint, it can trivially service local metadata read requests.

% remote and local reading
Since a file's metadata includes its web URL, reading a remote file's data is simply a matter of using $T$ to transform the URL $U$ into it's CDN-specific URL $U_CDN$ and downloading the file from $U_CDN$.  Reads on remote files are synchronous and will block until the requested data range requested has been fetched (or the download fails due to an irrecoverable fault).  Read operations on locally-hosted files are simply redirected to the underlying local storage mechanism that holds the data directory.

Whenever a file is downloaded in its entirety from a read request, the client may optionally calculate a cryptographic hash of the file data and compare it against the hash given in the metadata.  The read request and all subsequent read/write requests to that file will fail if the hashes do not match, for as long as the handle to the file is open.

% local file writes
The client processes a write operation~\endnote{Write operations include making changes to a file's metadata and/or a file's data, as well as file creation and unlinking.} on a locally-hosted file by simply writing the data to local storage.  However, the subsequent responses to remote requests for this file's data will be marked as uncacheable to the CDN.

When the handle to the file is closed, it \textit{republishes} the file to the CDN by marking responses to the current URLs to the file as un-cacheable, making a new, unique URL for the file, calculating new metadata for the file, and sending the metadata to the metadata server.  Republishing a file is an atomic operation, but to minimize the latency of the operation, the client informs the writer that its handle has closed before it informs the metadata server of the change.

% metadata posting
Metadata changes are buffered and periodically but asynchronously uploaded in batches to the metadata server.  They are uploaded via HTTP(S), so they are sent in the order in which they occur and are received by the metadata server at most once.  Once an update has been successfully processed, the client \textit{withdraws} all old URLs to the file by simply ignoring future requests to them.  The entire process is detailed in Figure~\ref{write-operation}.

In the event that the client cannot upload metadata changes, it buffers them internally and continuously attempts to re-upload them until it succeeds.  The changes are uploaded in the order in which they occured locally, but are tagged with a hint that a previous upload failed so the metadata server can act accordingly.


\begin{figure}[h!]
\centering
\includegraphics[width=0.5\textwidth]{figures/write-operation}
\caption{Time diagram of writing, closing, and republishing a locally-hosted file \texttt{/work/text}.  The client is mounted on \texttt{/u/jude/syndicate/}, its data directory is \texttt{/u/jude/data/}, and its publish directory is \texttt{/var/www/syndicate/}.}
\label{write-operation}
\end{figure}

% remote file modifications
% TODO: this is horribly inefficient, and needs good justification (e.g. why didn't we have clients post their writes to other clients?)
The client handles a write operation on a remote file by making it a locally-hosted file first.  It synchronously downloads the file, optionally verifies its integrity with by calculating its cryptographic hash and comparing it to its known hash (if available) in the metadata, performs the write operation, and republishes it, changing its owner ID as well.  Future read requests to this file (if no other changes occur) will now be directed to this client.

% versioning
Each file in Syndicate has a version number, which Syndicate client uses to generate a unique URL to the file by appending it to the end.  It is initialized to 1 when the file is created, and resets to 1 if the file is deleted and recreated.  It is atomically incremented each time the Syndicate client republishes the file, and the metadata server verifies that each new URL to a file has a higher verison number than the previous.

% publishing remote files
To publish content from remote sources, the client allows the user to change the URL of a locally-hosted file by exposing the URL as an extended file attribute in the filesystem.  This way, a user may expose data stored on a latency- and bandwidth-constrained remote medium (such as Amazon S3)~\cite{S3}) as a file that can be distributed to ther clients via the underlying CDN.  This type of metadata change is propagated to the metadata server as if it were any other metadata write, but not all metadata fields for this file will be valid until a client attempts to write to it later (and ends up hosting the modified copy and generating metadata for it).


\subsection{The Metadata Server}

% what does a metadata server do?
The Syndicate metadata server maintains metadata for every file within a filesystem.  A metadata server defines a Syndicate ``volume", since each metadata server maintains an independent tree.  It maintains a model of the entire hierarchy's metadata called the \textit{master copy}.  The master copy is a filesystem hierarchy with the same directory structure as will be seen by clients, but each ``file" is a stub that contains only its serialized metadata to be served to clients.  The stub will be regenerated whenever new metadata is uploaded for it by a client.

% Metadata reading/writing
Each metadata server defines two URLs for its clients: a ``read" URL and a ``write" URL.  The read URL is used to fetch a directory's metadata from the master copy, including the metadata for its entries.  A client uses this data to recreate the directory and its entries locally.  Metadata uploaded to the metadata server via the write URL is validated and committed to the master copy.  A metadata server may optionally require user authentication in order to gain access to either URL.

% Write conflicts
Each metadata operation received is processed in the order in which it arrives.  In the event of multiple metadata writes to the same file, the last observed write succeeds.  So, if two clients A and B both commit write operations to it before they observe the other's write, a third client C will only see the effects A's or B's writes (but not both) when it goes to read the file after receiving the new metadata.  However, any client can determine which write succeeded by inspecting the URL and/or owner ID of a file once the metadata is refreshed.

% URL blacklisting and validation
URLs in Syndicate may be reused, such as when a file is deleted and then recreated.  To prevent a file from assuming a URL that refers to stale data, the metadata server keeps a blacklist of recently stale URLs which it uses to temporarily block metadata writes.  URLs in the blacklist are removed when the content associated with the URL in the CDN has been evicted.  This is necessary since the metadata server may host file metadata for unversioned content, and should try to prevent multiple versions of that content from being present on the CDN at one time.

% Content revalidation
To keep the filesystem consistent over time, the metadata server periodically revalidates the metadata quering the host of each file in the master copy order to verify that the file is still available.  If it is not, then the master copy entry for that file is removed, and it will disappear from the clients when they refresh their metadata.  Additionally, the metadata server verifies that each master copy entry corresponds to the most recent write operation it observed since the last validation.


\subsection{Fault Tolerance and Failure Modes}

The effect of a single client failure in the worst case is all of the local files it hosts will no longer be accessible.  They will temporarily be visible to other clients, but I/O operations that require the remote client to be contacted will fail with a sensible error code.  The metadata server will eventually detect that the files cannot be accessed when it revalidates the metadata, in which case it removes the file from the filesystem.  The underlying storage mechanism on the client's host may mitigate or eliminate this risk, however.

% TODO: make the remount read-only happen
If the metadata server fails, a client's attempts to refresh metadata will fail.  However, as long as the client runs, it will still provide a view of the filesystem as it was last known, and for a brief window of time local write operations will still succeed as far as the writers are concerned.  Other clients will not see the effects of other clients' writes as long as the metadata server is down, and the local writes that do succeed will be bufferred until they can be sent off.  The Syndicate client is configured to prevent future writes whenever it cannot contact the metadata server after a predetermined timeout in order to warn the user of this failure.

% TODO: make this happen
If the CDN fails to deliver data, all I/O operations on remote files will fail in every client.  To combat this, a client will fall back to reading the file directly from the file's remote host instead of going through the CDN.


\comment{ 
% NOTES

Semantically, you "mount" a metadata server via the Syndicate client.  The metadata server serves the client a filesystem hierarchy of metadata.
\\
The metadata for a file are its relative path, the URL it is available at, its modification time, its owner, its metadata server ID, access mode, size, and (optionally) its cryptographic checksum.
\\
The client creates a filesystem from the metadata and periodically refreshes it.  Agents can query the metadata via the usual syscalls.
\\
Reading a file involves downloading the file from its URL and copying its data into the caller's read buffer when it becomes available.
\\
When a file is created in the filesystem, its data gets stored locally and a version number gets appended to the ``local" path.  The version number is atomically incremented whenever a file handle
to the file that had been opened for writing is closed.
\\
When a file is written to in the filesystem, the file is already hosted locally, or the file is hosted remotely.  If it is local, then the write happens in the
usual way and when the handle is closed the version number gets incremented and the metadata server is informed.
If the file is hosted remotely, the client downloads the file locally and then writes the data to the local copy.
\\
Clients periodically re-download new metadata from the metadata server.
\\
\subsection{Metadata Server}
The metadata server manages the metadata of a filesystem hierarchy.  Whenever a client modifies a file, it uploads the new metadata to the metadata server.
\\
The metadata server performs write-conflict resolution by simply taking the last write to a particular file as the write that succeeded.  
Whenever a client performs a write, the metadata server is informed (since the URL to the file will have changed, since its version number incremented).
\\
The metadata server periodically validates each piece of metadata server by querying each URL for each file and seeing that it is still valid.
\\
\subsection{Content Distribution Network}
TAKEAWAY 1: The reader should have a super-high-level idea of how CoBlitz works, and why now CoBlitz was chosen for our CDN.
\\
Overview of CoBlitz design choices.
\\
CoBlitz nodes cache chunks of requested objects, not the whole object.
\\
CoBlitz replicates chunks between its nodes using highest random weighting.
\\
CoBlitz whitelists clients to contribute to the filesystem.
\\
CoBlitz redirects clients to the appropriate CoBlitz node, based on the requester's location and the health of the nodes.
\\
CoBlitz is already known to be scalable in both the amount of data and number of clients, and has excellent throughput. 
\\
}


