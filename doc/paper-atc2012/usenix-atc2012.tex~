% TEMPLATE for Usenix papers, specifically to meet requirements of
%  USENIX '05
% originally a template for producing IEEE-format articles using LaTeX.
%   written by Matthew Ward, CS Department, Worcester Polytechnic Institute.
% adapted by David Beazley for his excellent SWIG paper in Proceedings,
%   Tcl 96
% turned into a smartass generic template by De Clarke, with thanks to
%   both the above pioneers
% use at your own risk.  Complaints to /dev/null.
% make it two column with no page numbering, default is 10 point

% Munged by Fred Douglis <douglis@research.att.com> 10/97 to separate
% the .sty file from the LaTeX source template, so that people can
% more easily include the .sty file into an existing document.  Also
% changed to more closely follow the style guidelines as represented
% by the Word sample file. 

% Note that since 2010, USENIX does not require endnotes. If you want
% foot of page notes, don't include the endnotes package in the 
% usepackage command, below.

% This version uses the latex2e styles, not the very ancient 2.09 stuff.

% NOTE: use end-notes, not footnotes

% Citation example:
% Now we're going to cite somebody.  Watch for the cite tag.
% Here it comes~\cite{Chaum1981,Diffie1976}.  The tilde character (\~{})
% in the source means a non-breaking space.  This way, your reference will
% always be attached to the word that preceded it, instead of going to the
% next line.

\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix,epsfig,endnotes,graphicx,float,url,longtable}
\newcommand{\comment}[1]{}
\begin{document}

%don't want date printed
\date{}

%make title bold and 14 pt font (Latex default is non-bold, 16 pt)
\title{\Large \bf Syndicate: A Scalable, CDN-powered Distributed Read-Write Filesystem}

%for single author (just remove % characters)
\author{
{\rm Jude Nelson}\\
Princeton University
\and
{\rm Larry Peterson}\\
Princeton University
% copy the following lines to add more authors
% \and
% {\rm Name}\\
%Name Institution
} % end author

\maketitle

% Use the following at camera-ready time to suppress page numbers.
% Comment it out when you first submit the paper for review.
\thispagestyle{empty}
	

\subsection*{Abstract}
Syndicate abstract

\input{introduction}

\input{design}

\input{implementation}

\section{Evaluation}
Evaluation of Syndicate.
\subsection{Experimental Design}
Put CoBlitz, CoRedirect, DnsDemux in VICCI slices
\\
Put Syndicate clients in a separate VICCI slice.
\\
Put Syndicate metadata servers in a separate VICCI slice, managed by the Syndicate Metadata Service.
\\
We do NOT want Syndicate clients and metadata servers to be colocated, nor do we want Syndicate clients to be colocated with CoBlitz nodes.  CoBlitz nodes should be at least one network hop away, since we want to simulate the effect of running Syndicate clients with CoBlitz nodes as if they were all in the same ISP and the ISP happened to have a Coblitz PoP.  We can put some Syndicate clients in PlanetLab slivers that are a small number (i.e. less than 5) hops away from a Coblitz node.
\\
One possibility is to put CoBlitz in nodeXX.princeton.vicci.org slivers; put the Syndicate slice in nodeXX.gt.vicci.org slivers; put the metadata slice in nodeXX.washington.vicci.org slivers. 
\\
NOTE: CERN VM-fs does NOT have any published data on it yet.  TODO: find ``canonical" filesystem benchmarks.
\\
Possible experiments:
\begin{itemize}
\item Throughput bar graph of syndicate client on reading/writing local files versus a "stub" FUSE filesystem versus ext3.
\item Mean/Median/99th percentile download speeds of a variable-sized file (1GB, 10GB, 100GB, 1TB, etc.) to a single client (cold cache performance)
\item Mean/Median/99th percentile download speeds of a large file to a varied number of clients (10, 100, 1000, etc.) (warm cache performance)
\item Run a few scientific computations on every client on large datasets hosted on a few clients.  Compare timings to AFS, NFS, Shark, CERN VM-fs.
\end{itemize}

More possible experiments (if we have time)
\begin{itemize}
\item AFS benchmark average/standard-deviation on Syndicate versus CERN VM-fs, Logistical FS, Shark, AFS.
\item See how the system performs when there are \textit{i} different files in transfer simultaneously to a fixed number of clients.  How does Syndicate perform under load?  How does the CDN perform when it is thrashing?
\item Compare Syndicate performance with/without a CDN for having a variable (10, 100, 1000, etc.) number of clients download the same file from another client.
\end{itemize}

\section{Related Work}
Logicistal Filesystem (read-write; uses mirroring (file depots) for scalability) \\
CERN VM-fs (uses Squid hierarchy for caching; read-only) \\
Shark (nodes do caching on behalf of other nodes; supports encryption) \\
AFS, Coda, Ceph, Lustre, etc. (no inter-node caching by default) \\

\section{Conclusion}
Conclusion of paper

\section{Acknowledgements}
Sapan Bhatia \\
Andy Bavier \\
Verivue (especially the Princeton Office developers)


{\footnotesize \bibliographystyle{acm}
%\bibliography{../common/bibliography}}


\theendnotes

\end{document}







